{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author:  Leo Pauly (cnlp@leeds.ac.uk) & Nick Wilson (n.wilson@lubs.leeds.ac.uk)\n",
      "Description: Autmatic database update\n"
     ]
    }
   ],
   "source": [
    "print('Author:  Leo Pauly (cnlp@leeds.ac.uk) & Nick Wilson (n.wilson@lubs.leeds.ac.uk)')\n",
    "print('Description: Autmatic database update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage intructions & Other info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Give absolute file paths always\n",
    "2. Assumed that new entry directory names always start with UKLTD_R_ or UKLTD_W_\n",
    "3. File dir_list.txt contains the processed directory list\n",
    "4. Prefefably install python using anacodna (all in one installation): https://www.anaconda.com/products/individual#windows\n",
    "5. Run this if 'pyreadstat' module is missing':  !pip install pyreadstat \n",
    "6. Need only change base directory in most of the case\n",
    "7. Databases are stored in the directory (Create one if missing) : UKLTD_Database\n",
    "8. Select user (user variable options: 'leo','nick')\n",
    "9. Run this if 'patool' module is missing':  !pip install patool pyunpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info:\n",
      "Python version: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "No: of logical processors: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "from zipfile import ZipFile\n",
    "import pyunpack\n",
    "import multiprocessing\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "print('Info:')\n",
    "print('Python version:',sys.version)\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print('No: of logical processors:',num_processes)\n",
    "\n",
    "AC01_header={'AC01': 'string', 'REGNUM':'float64' , 'ACCDAT_start': 'string', 'ACCDAT': 'string', 'number_of_weeks': 'string', 'months': 'string',\n",
    "       'currency': 'string', 'consolidated': 'string', 'acctype': 'float64', 'Turnover': 'float64', 'Export': 'float64',\n",
    "       'Cost_of_Sales': 'float64', 'Gross_Profit': 'float64', 'Wages_Salaries': 'float64',\n",
    "       'Directors_Emoluments': 'float64', 'Operating_Profits': 'float64', 'Depreciation': 'float64',\n",
    "       'Audit_Fees': 'float64', 'Interest_Payments': 'float64', 'Pre_Tax_Profit': 'float64', 'taxation1': 'float64',\n",
    "       'Profit_After_Tax': 'float64', 'Dividends_Payable': 'float64', 'Retained_Profits': 'float64',\n",
    "       'Tangible_Assets': 'float64', 'Intangible_Assets': 'float64', 'Total_Fixed_Assets': 'float64',\n",
    "       'Total_Current_Assets': 'float64', 'Trade_Debtors': 'float64', 'Stock': 'float64', 'Cash': 'float64',\n",
    "       'Other_Current_Assets': 'float64', 'Increase_In_Cash': 'float64', 'Mis_Current_Assets': 'float64',\n",
    "       'Total_Assets': 'float64', 'Total_Current_Liabilities': 'float64', 'Trade_Creditors': 'float64',\n",
    "       'Bank_Overdraft': 'float64', 'Other_Short_Term_Fin': 'float64', 'Mis_Current_Liabilities': 'float64',\n",
    "       'Other_Long_Term_Fin': 'float64', 'Total_Long_Term_Liabilities': 'float64',\n",
    "       'Bank_Overdraft_LTL': 'float64', 'Total_Liabilities': 'float64', 'Net_Assets': 'float64',\n",
    "       'Working_Capital': 'float64', 'Paid_up_equity': 'float64', 'PL_Account_Weserve': 'float64',\n",
    "       'Sundry_Weserves': 'float64', 'Revaluation_Weserve': 'float64', 'Shareholder_Funds': 'float64',\n",
    "       'NetWorth': 'float64', 'NetCashflowfromOperations': 'float64', 'NetCashflowbeforeFinancing': 'float64',\n",
    "       'NetCashflowfromFinancing': 'float64', 'Contingent_Liability': 'float64', 'Capital_Employed': 'float64',\n",
    "       'No_Employees': 'float64', 'status': 'float64', 'UPLOAD':'string'}\n",
    "\n",
    "AC01_header_names=list(AC01_header.keys())\n",
    "AC01_header_dtypes={'Bank_Overdraft': 'float64',\n",
    "       'Bank_Overdraft_LTL': 'float64',\n",
    "       'Capital_Employed': 'float64',\n",
    "       'Mis_Current_Assets': 'float64',\n",
    "       'NetCashflowbeforeFinancing': 'float64',\n",
    "       'NetWorth': 'float64',\n",
    "       'Net_Assets': 'float64',\n",
    "       'REGNUM': 'object',\n",
    "       'Tangible_Assets': 'float64',\n",
    "       'acctype': 'float64',\n",
    "       'status': 'float64'}#list(AC01_header.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selectting user and adding filepaths\n",
    "user='leo'\n",
    "if(user=='leo'):\n",
    "    base_dir='C:/Users/cnlp/Research Fellowship/'\n",
    "    dir_list_file=base_dir+'/UKLTD_Scripts/dir_list.txt'\n",
    "    database_file=base_dir+'UKLTD_Database/AC01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monitering\n",
    "#client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking new entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of processed directores:\n",
      "\n",
      "\n",
      "New entries detected:\n",
      "UKLTD_W_20190602\n"
     ]
    }
   ],
   "source": [
    "## Checking for new download\n",
    "with open(dir_list_file, 'a+') as fd:\n",
    "    fd.seek(0)\n",
    "    dir_list_old=fd.read().split('\\n')\n",
    "\n",
    "print('List of processed directores:',*dir_list_old,sep='\\n')\n",
    "dir_list_new=[dir_name for dir_name in os.listdir(base_dir) if (dir_name.startswith(\"UKLTD_W\") or dir_name.startswith(\"UKLTD_R\"))]\n",
    "entry_dir_list=[entry_dir for entry_dir in dir_list_new if entry_dir not in dir_list_old ]\n",
    "print('\\nNew entries detected:',*entry_dir_list,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating/Loading database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database not found. Creating database! \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_database' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-78b0c2462963>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#df_database=dd.from_pandas(df_database_,npartitions=1*multiprocessing.cpu_count())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdf_database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_database' is not defined"
     ]
    }
   ],
   "source": [
    "## If databse is in .csv format \n",
    "if (os.path.isfile(database_file)):\n",
    "    print('Database found.','Reading database! \\n') \n",
    "    start = time.process_time()\n",
    "    df_database = dd.read_csv(database_file,assume_missing=True) \n",
    "    print('Time taken to read database {}:'.format(database_file.split('/')[-1]),time.process_time() - start,'s')\n",
    "    df_database\n",
    "else:\n",
    "    print('Database not found.','Creating database! \\n')\n",
    "    first_time=True\n",
    "    #df_database_=pd.DataFrame(columns=AC01_header_names)\n",
    "    #df_database=dd.from_pandas(df_database_,npartitions=1*multiprocessing.cpu_count())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules:\n",
    "\n",
    "1. Check for field:REGNUM\n",
    "2. If REGNUM doesn't exist: add as new row in database\n",
    "3. If they exist, check for the rest of the fields\n",
    "4. If they are not identical: add as new row in database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(entry_dir):\n",
    "    global df_database\n",
    "    entry_file=\"AC01_\"+(entry_dir.split('_')[-2])+\"_\"+(entry_dir.split('_')[-1])+\".txt\"\n",
    "    entry_file_zip=\"AC01_\"+(entry_dir.split('_')[-2])+\"_\"+(entry_dir.split('_')[-1])+\".rar\"\n",
    "    pyunpack.Archive(base_dir+'{}/{}'.format(entry_dir,entry_file_zip)).extractall(base_dir+'{}/'.format(entry_dir))\n",
    "    print('Entry file unzipped as:',entry_file)\n",
    "    \n",
    "    start = time.process_time()\n",
    "    df_entry_file=dd.read_csv('C:/Users/cnlp/Research Fellowship/{}/{}'.format(entry_dir,entry_file),sep='|',names=AC01_header_names,dtype=AC01_header_dtypes)\n",
    "    print('Time taken to read:',time.process_time() - start,'s')\n",
    "    df_entry_file['UPLOAD']=entry_file.split('.')[0]\n",
    "        \n",
    "    start = time.process_time()\n",
    "    if(not first_time):\n",
    "        df_database=df_database.merge(df_entry_file)\n",
    "    else:\n",
    "        df_database=df_entry_file\n",
    "    print('Time taken to process:',time.process_time() - start,'s')\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Reading from new directory\n",
    "for entry_dir in entry_dir_list:\n",
    "    print('\\nReading from entry dir:',entry_dir)\n",
    "    print('Update Success:',update_database(entry_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing updated databse to file\n",
    "print('Database {} updated'.format(database_file.split('/')[-1]))\n",
    "df_database.to_csv(database_file,single_file = True,assume_missing=True) #pyreadstat.write_sav(df_database,database_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update processed directory list\n",
    "print('Processed list updated')\n",
    "with open(dir_list_file, 'w') as fd:\n",
    "    fd.write('\\n'.join(dir_list_old+entry_dir_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
